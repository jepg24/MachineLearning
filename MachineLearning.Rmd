---
title: "Final Project - Practical Machine Learning"
author: "Jesús Pérez"
date: "3/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE)

library(caret)
library(rattle)
library(corrplot)
library(gbm)
 
```

# Introduction

This project use data from accelerometers on the belt, forearm, arm, and dumbell of 6 research participants. The participants were each instructed to perform an exercise, and the result was classified either as properly performed (Class A) or in four common weightlifting mistakes (Classes B, C, D, and E). Source: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har>. 

The aim of this project is to evaluate diffent Machine Learning Model using a **Train** set and apply the best model to predict the labels for another **Test** set observations.

# Preparation of the Data

## Reading Data

As shown below there are 19622 observations and 160 variables in the **Train** dataset, and 20 observations and the same number of varibles in the **Test** dataset.

```{r Open data}
File1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
File2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

TrainData <- read.csv(File1)
TestData <- read.csv(File2)

dim(TrainData)
dim(TestData)
```

## Cleaning Data

There were a lot of dummy variables in the original datasets, and several columns did not have measurements for each observation. For this reason, the variables that contains missing values were removed. All rows containing summary statistics were removed as well. This reduced considerably the size of the datasets, especially the **Train** set.

```{r clean data, eval=TRUE}

Train <- TrainData[, colSums(is.na(TrainData)) == 0]
Test <- TestData[, colSums(is.na(TestData)) == 0]

Train <- Train[, -c(1:7)]
Test <- Test[, -c(1:7)]

dim(Train); dim(Test)
```

The **Train** set was further claened by removing the variables that are near-zero-variance. This equaled the number of variables in both datasets.

```{r cleaning more}
NZV <- nearZeroVar(Train)
Train <- Train[, -NZV]
dim(Train)
```

## Correlation Analysis

The correlation among variables was analysed before proceeding to the modeling procedures.

````{r Exploring the data, fig.cap = "Correlation Matrix"}
corMatrix <- cor(Train[, -53])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

The results showed that some of the variables were highly correlated (more than 0.9). Those variables were also removed.

```{r correlation}
corr <- findCorrelation(corMatrix, cutoff = .90)
Train <- Train[, -corr]
Test <- Test[, -corr]
```

# Model Building

Before starting to fit the models, a *cross validation* was carried out to the the **Train** model. That dataset was splited in two, a new **Train** dataset which contained the 70% of the data and a **Validation** dataset with the remaining 30%.

```{r Cross Validation}
set.seed(4321)
inTrain <- createDataPartition(Train$classe, p = 0.7, list= FALSE)
Train <- Train[inTrain, ]
Validation <- Train[-inTrain, ]
````

In this way, three methods were applied to model the regressions (in the **Train** dataset) and the best one (higher accuracy) was used for the prediction in the **Test** set. The methods were: 

*Decision Tree
*Random Forests
*Generalized Boosted Model

For each one, a confusion Matrix was showed to better visualize the accuracy of the models.

# Decision Tree

```{r Trees}
control <- trainControl(method="cv", number=3, verboseIter=F)
trees <- train(classe~., data=Train, method="rpart", trControl = control, tuneLength = 5)
fancyRpartPlot(trees$finalModel)
```

**Validation**

```{r Prediction Tree}
Prediction_trees <- predict(trees, Validation)
CM_trees <- confusionMatrix(Prediction_trees, factor(Validation$classe))
CM_trees
```

# Random forest

```{r Random Forest}
controlRF <- trainControl(method="cv", number=3, verboseIter=F)
RandomForest <- train(classe~., data=Train, method="rf", trControl = controlRF)
RandomForest$finalModel
```

**Validation**

```{r Prediction Random Forest}
Prediction_RandomForest <- predict(RandomForest, Validation)
CM_RandomForest <- confusionMatrix(Prediction_RandomForest, factor(Validation$classe))
CM_RandomForest
```

# Generalized Boosted Model

```{r Generalized Boosted Model}
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
GBM  <- train(classe ~ ., data=Train, method = "gbm", trControl = controlGBM, verbose = FALSE)
GBM$finalModel
```

**Validation**

```{r Prediction GBM}
Prediction_GBM <- predict(GBM, newdata=Validation)
CM_GBM <- confusionMatrix(Prediction_GBM, factor(Validation$classe))
CM_GBM
```

## Comparison

By comparing the accuracy rate values of the three models, the *Random Forest* displayed the higher accuracy. For this reason, this model was used on the **Test** data to predict the *classe* variable.

```{r Comparison}
Table <- data.frame(CM_trees$overall, CM_RandomForest$overall, CM_GBM$overall)
Table[-(5:7), ]
```

# Prediction 

Finally, the Random Forest Model was used to predict the outcome of the **Test** set. These results will be used to answer the “*Course Project Prediction Quiz*”.

```{r Prediction TEST}
Predict <- predict(RandomForest, Test)
Predict
```
